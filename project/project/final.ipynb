{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import re\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file and return a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The loaded dataset.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Data Preprocessing and Feature Extraction\n",
    "def preprocess_and_extract_features(data, top_n_terms):\n",
    "    \"\"\"\n",
    "    Preprocess the data and extract features.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The input dataset containing 'url' and 'status' columns.\n",
    "    top_n_terms (int): The number of top terms to extract.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The feature matrix.\n",
    "    \"\"\"\n",
    "    # Balance the dataset (your code for balancing the data)\n",
    "\n",
    "    # Tokenize URLs and extract top terms\n",
    "    vocab = top_terms(data, top_n_terms)\n",
    "\n",
    "    CORPUS = [to_txt(url) for url in data.url]\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True, vocabulary=vocab)\n",
    "    docTermMatrix = vectorizer.fit_transform(CORPUS)\n",
    "\n",
    "\n",
    "    # Transform URLs into a feature matrix\n",
    "    feature_matrix = create_feature_matrix(data, docTermMatrix, vocab, CORPUS)\n",
    "\n",
    "    return feature_matrix\n",
    "\n",
    "def top_terms(data, top_n_terms):\n",
    "    \"\"\"\n",
    "    Extract the top 'n' terms based on their frequency in the dataset.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The dataset with 'url' and 'status' columns.\n",
    "    top_n_terms (int): The number of top terms to extract.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of top terms.\n",
    "    \"\"\"\n",
    "    term_frequency = {}\n",
    "    for url, status in data.values:\n",
    "        for word in tokenize_url(url):\n",
    "            if word != '':\n",
    "                if word not in term_frequency:\n",
    "                    term_frequency[word] = 0\n",
    "                term_frequency[word] += 1\n",
    "            \n",
    "    top_terms = [term for term, _ in sorted(term_frequency.items(), key=lambda x: x[1], reverse=True)[:top_n_terms]]\n",
    "    return top_terms\n",
    "\n",
    "def tokenize_url(url):\n",
    "    \"\"\"\n",
    "    Tokenize a URL by replacing slashes with dots and splitting on dots.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of terms (words).\n",
    "    \"\"\"\n",
    "    return url.replace('/', '.').split('.')\n",
    "\n",
    "\n",
    "def to_txt(text) -> str:\n",
    "    return text.replace('.', ' ').replace('/', ' ')\n",
    "\n",
    "def num_digits(text) -> int:\n",
    "    return len(re.findall('\\d', text))\n",
    "\n",
    "def num_dots(text) -> int:\n",
    "    return len(re.findall('\\.', text))\n",
    "\n",
    "def num_bar(text) -> int:\n",
    "    return len(re.findall('/', text))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_feature_matrix(df_balanced,docTermMatrix, VOC, CORPUS):\n",
    "    \"\"\"\n",
    "    Create a feature matrix based on the provided vocabulary.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The dataset with 'url' column.\n",
    "    vocabulary (list): The list of terms to use as features.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The feature matrix.\n",
    "    \"\"\"\n",
    "    # Additional preprocessing (your code for feature extraction)\n",
    "\n",
    "  \n",
    "\n",
    "    matrix = pd.DataFrame(docTermMatrix.A, columns=VOC)\n",
    "    matrix['dots'] = [num_dots(text) for text in df_balanced.url]\n",
    "    matrix['bar'] = [num_bar(text) for text in df_balanced.url]\n",
    "    matrix['len'] = [len(text) for text in CORPUS]\n",
    "    matrix['digits'] = [num_digits(text) for text in CORPUS]\n",
    "\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def create_feature(data, vocabulary):\n",
    "    \"\"\"\n",
    "    Create a feature matrix based on the provided vocabulary.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The dataset with 'url' column.\n",
    "    vocabulary (list): The list of terms to use as features.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The feature matrix.\n",
    "    \"\"\"\n",
    "    # Tokenize URLs in the 'url' column\n",
    "    data['url'] = data['url'].apply(tokenize_url)\n",
    "\n",
    "    # Join the tokenized terms into a single string for each URL\n",
    "    data['url'] = data['url'].apply(lambda terms: ' '.join(terms))\n",
    "\n",
    "    # Create a CountVectorizer with binary=True to represent the presence or absence of terms\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary, binary=True)\n",
    "\n",
    "    # Transform the tokenized URLs into a feature matrix\n",
    "    feature_matrix = vectorizer.transform(data['url'])\n",
    "\n",
    "    # Convert the feature matrix to a DataFrame for better readability\n",
    "    feature_matrix_df = pd.DataFrame(data=feature_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return feature_matrix_df\n",
    "\n",
    "# Model Training and Evaluation\n",
    "def train_and_evaluate_model(feature_matrix, labels):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier and evaluate its performance.\n",
    "\n",
    "    Args:\n",
    "    feature_matrix (pd.DataFrame): The feature matrix.\n",
    "    labels (pd.Series): The target labels (statuses).\n",
    "\n",
    "    Returns:\n",
    "    RandomForestClassifier: The trained Random Forest classifier.\n",
    "    float: The accuracy of the model.\n",
    "    np.ndarray: The confusion matrix.\n",
    "    \"\"\"\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a Random Forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return clf, accuracy, confusion\n",
    "\n",
    "\n",
    "# Classification of a Single URL\n",
    "def classify_url(url, model, feature_names):\n",
    "    \"\"\"\n",
    "    Classify a single URL as phishing or clean.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to classify.\n",
    "    model (RandomForestClassifier): The trained Random Forest classifier.\n",
    "    feature_names (list): List of feature names used in the model.\n",
    "\n",
    "    Returns:\n",
    "    str: The predicted class ('phishing' or 'clean').\n",
    "    \"\"\"\n",
    "    # Preprocess the URL to match the format used in the feature matrix\n",
    "    url_text = to_txt(url)\n",
    "\n",
    "    # Create a feature matrix for the single URL\n",
    "    single_url_features = create_feature(pd.DataFrame({'url': [url_text]}), feature_names)\n",
    "\n",
    "    # Predict the class\n",
    "    prediction = model.predict(single_url_features)\n",
    "\n",
    "    # Map the numeric class to 'phishing' or 'clean'\n",
    "    return 'phishing' if prediction[0] == 1 else 'clean'\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    dataset = load_data(r'C:\\Users\\Wolfred\\Documents\\Data Analysis Boot Camp\\bootcamp_project4\\project\\project\\new_data_urls.csv')\n",
    "    # print(len(df[df['status']==0]), len(df[df['status']==1]))\n",
    "    df_maj, df_min = dataset[dataset['status']==1], dataset[dataset['status']==0]\n",
    "    df_maj_sampled = df_maj.sample(len(df_min), random_state=42)\n",
    "    df_balanced = pd.concat([df_maj_sampled, df_min])\n",
    "    # print(len(df_balanced[df_balanced['status']==0]), len(df_balanced[df_balanced['status']==1]))\n",
    "    df_balanced.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "    # Define the number of top terms to extract\n",
    "    top_n_terms = 10\n",
    "\n",
    "    # Preprocess data and extract features\n",
    "    feature_matrix = preprocess_and_extract_features(df_balanced, top_n_terms)\n",
    "\n",
    "    # Train and evaluate the Random Forest model\n",
    "    model, accuracy, confusion = train_and_evaluate_model(feature_matrix, df_balanced['status'])\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "\n",
    "\n",
    "    # Interactive classification\n",
    "    while True:\n",
    "        url_to_classify = input(\"Enter a URL to classify (or 'exit' to quit): \")\n",
    "        if url_to_classify.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        classification = classify_url(url_to_classify, model, feature_matrix.columns)\n",
    "        print(f\"Classification for '{url_to_classify}': {classification}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8884444247529953\n",
      "Confusion Matrix:\n",
      "[[69176  9828]\n",
      " [ 7797 71192]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000111servicehelpdesk.godaddysites.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000011accesswebform.godaddysites.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003.online</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0009servicedeskowa.godaddysites.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000n38p.wcomhost.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822005</th>\n",
       "      <td>zzufg.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822006</th>\n",
       "      <td>zzu.li</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822007</th>\n",
       "      <td>zzz.co.uk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822008</th>\n",
       "      <td>zzzoolight.co.za</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822009</th>\n",
       "      <td>zzzoolight.co.za0-i-fdik.000webhostapp.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>822010 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               url  status\n",
       "0          0000111servicehelpdesk.godaddysites.com       0\n",
       "1             000011accesswebform.godaddysites.com       0\n",
       "2                                     00003.online       0\n",
       "3              0009servicedeskowa.godaddysites.com       0\n",
       "4                             000n38p.wcomhost.com       0\n",
       "...                                            ...     ...\n",
       "822005                                   zzufg.com       0\n",
       "822006                                      zzu.li       0\n",
       "822007                                   zzz.co.uk       0\n",
       "822008                            zzzoolight.co.za       0\n",
       "822009  zzzoolight.co.za0-i-fdik.000webhostapp.com       0\n",
       "\n",
       "[822010 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    dataset = load_data(r'C:\\Users\\Wolfred\\Documents\\Data Analysis Boot Camp\\bootcamp_project4\\project\\project\\new_data_urls.csv')\n",
    "    # print(len(df[df['status']==0]), len(df[df['status']==1]))\n",
    "    df_maj, df_min = dataset[dataset['status']==1], dataset[dataset['status']==0]\n",
    "    df_maj_sampled = df_maj.sample(len(df_min), random_state=42)\n",
    "    df_balanced = pd.concat([df_maj_sampled, df_min])\n",
    "    # print(len(df_balanced[df_balanced['status']==0]), len(df_balanced[df_balanced['status']==1]))\n",
    "    df_balanced.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "load_data(r'C:\\Users\\Wolfred\\Documents\\Data Analysis Boot Camp\\bootcamp_project4\\project\\project\\new_data_urls.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
